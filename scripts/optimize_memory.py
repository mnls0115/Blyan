#!/usr/bin/env python3
"""
ë©”ëª¨ë¦¬ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
Memory optimization script for AI model loading
"""

import torch
import gc
import psutil
import json
import sys
from pathlib import Path

class MemoryOptimizer:
    def __init__(self):
        self.config_file = Path("./config/memory_config.json")
        self.load_config()
    
    def load_config(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ë¡œë“œ"""
        if self.config_file.exists():
            with open(self.config_file) as f:
                self.config = json.load(f)
        else:
            # ê¸°ë³¸ ì„¤ì •
            self.config = {
                "max_model_memory_gb": 4.0,
                "expert_cache_limit_gb": 2.0,
                "inference_batch_size": 1,
                "enable_gradient_checkpointing": True,
                "use_cpu_offload": True,
                "torch_cache_limit_gb": 1.0
            }
            self.save_config()
    
    def save_config(self):
        """ì„¤ì • ì €ì¥"""
        self.config_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_file, 'w') as f:
            json.dump(self.config, f, indent=2)
    
    def get_available_memory_gb(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (GB) ê³„ì‚°"""
        memory = psutil.virtual_memory()
        return memory.available / (1024**3)
    
    def optimize_torch_settings(self):
        """PyTorch ë©”ëª¨ë¦¬ ìµœì í™”"""
        print("ğŸ”§ PyTorch ë©”ëª¨ë¦¬ ìµœì í™” ì¤‘...")
        
        # CUDA ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            # CUDA ë©”ëª¨ë¦¬ í• ë‹¹ ë°©ì‹ ìµœì í™”
            torch.backends.cudnn.benchmark = False  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
            torch.backends.cudnn.deterministic = True
        
        # CPU ë©”ëª¨ë¦¬ ìµœì í™”
        torch.set_num_threads(2)  # CPU ìŠ¤ë ˆë“œ ì œí•œìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        print(f"âœ… PyTorch ì„¤ì • ì™„ë£Œ")
        print(f"   - CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        print(f"   - CPU ìŠ¤ë ˆë“œ: {torch.get_num_threads()}")
    
    def update_model_config(self):
        """ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸"""
        available_memory = self.get_available_memory_gb()
        
        # ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë™ì  ì„¤ì •
        if available_memory < 8:
            self.config.update({
                "max_model_memory_gb": min(2.0, available_memory * 0.25),
                "expert_cache_limit_gb": min(1.0, available_memory * 0.125),
                "inference_batch_size": 1
            })
        elif available_memory < 16:
            self.config.update({
                "max_model_memory_gb": min(4.0, available_memory * 0.25),
                "expert_cache_limit_gb": min(2.0, available_memory * 0.125),
                "inference_batch_size": 2
            })
        else:
            self.config.update({
                "max_model_memory_gb": min(8.0, available_memory * 0.3),
                "expert_cache_limit_gb": min(4.0, available_memory * 0.2),
                "inference_batch_size": 4
            })
        
        self.save_config()
        print(f"ğŸ“ ë©”ëª¨ë¦¬ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì‚¬ìš© ê°€ëŠ¥: {available_memory:.1f}GB)")
        print(f"   - ìµœëŒ€ ëª¨ë¸ ë©”ëª¨ë¦¬: {self.config['max_model_memory_gb']}GB")
        print(f"   - ì „ë¬¸ê°€ ìºì‹œ í•œë„: {self.config['expert_cache_limit_gb']}GB")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {self.config['inference_batch_size']}")
    
    def setup_monitoring_script(self):
        """ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰"""
        import subprocess
        import os
        
        script_path = Path(__file__).parent / "memory_monitor.py"
        
        # Python ê²½ë¡œ ì°¾ê¸°
        python_paths = [
            f"{os.getcwd()}/myenv/bin/python",     # í˜„ì¬ ë””ë ‰í† ë¦¬ ê°€ìƒí™˜ê²½
            f"{os.getcwd()}/.venv/bin/python",     # .venv ê°€ìƒí™˜ê²½
            sys.executable,                         # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Python
            "/usr/bin/python3",                     # ì‹œìŠ¤í…œ Python3
            "python3"                               # PATHì˜ Python3
        ]
        
        python_exe = None
        for path in python_paths:
            try:
                if Path(path).exists() or subprocess.run(["which", path], capture_output=True).returncode == 0:
                    python_exe = path
                    break
            except:
                continue
        
        if not python_exe:
            print(f"âš ï¸ Python ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        try:
            # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            process = subprocess.Popen(
                [python_exe, str(script_path)],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            
            # PID ì €ì¥
            pid_file = Path("./logs/memory_monitor.pid")
            pid_file.parent.mkdir(exist_ok=True)
            with open(pid_file, 'w') as f:
                f.write(str(process.pid))
            
            print(f"ğŸ” ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘ë¨ (PID: {process.pid})")
            return True
            
        except Exception as e:
            print(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return False

def main():
    optimizer = MemoryOptimizer()
    
    print("ğŸš€ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œì‘...")
    print(f"ğŸ’¾ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {optimizer.get_available_memory_gb():.1f}GB")
    
    # ìµœì í™” ì‹¤í–‰
    optimizer.optimize_torch_settings()
    optimizer.update_model_config()
    
    # ëª¨ë‹ˆí„°ë§ ì‹œì‘
    optimizer.setup_monitoring_script()
    
    print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ!")

if __name__ == "__main__":
    main()