#!/usr/bin/env python3
"""
Dataset Block Structure for Chain D - Training Data Governance

This module defines the structure and validation logic for dataset blocks
in the Blyan Dataset-Chain (Chain D), enabling complete transparency and
community governance of AI training data.
"""

import hashlib
import json
import time
from dataclasses import dataclass, asdict
from typing import Optional, List, Dict, Any, Literal
from enum import Enum

from .block import Block, BlockHeader


class DatasetQualityTier(Enum):
    """Dataset quality classification tiers."""
    GOLD = "gold"           # Verified high-quality, legally cleared
    SILVER = "silver"       # Community-approved, good reputation
    EXPERIMENTAL = "experimental"  # New/unverified, use with caution
    QUARANTINED = "quarantined"    # Community-flagged, avoid usage


class DatasetStage(Enum):
    """4-stage dataset lifecycle pipeline."""
    PENDING = "pending"         # Stage 1: Just uploaded, awaiting auto-audit
    AUTO_AUDIT = "auto_audit"   # Stage 2: AI Quality Gate processing (â‰¤30 min)
    COMMUNITY_VOTE = "community_vote"  # Stage 3: DAO governance (72 hours)
    APPROVED = "approved"       # Stage 4: Classified and ready for training
    REJECTED = "rejected"       # Failed quality checks or community vote


@dataclass
class QualityReport:
    """Auto-generated quality assessment report."""
    
    # Core quality metrics (generated by AI Quality Gate)
    toxicity: float = 0.0           # 0.0-1.0, toxic content percentage
    duplicate_rate: float = 0.0     # 0.0-1.0, similarity to existing datasets
    pii_detected: bool = False      # Personal information found
    lang_ratio: Dict[str, float] = None  # Language distribution {"en": 0.9, "ko": 0.1}
    perplexity_improvement: float = 0.0  # Expected model improvement
    
    # Legal compliance
    license_verified: bool = False   # License automatically verified
    copyright_hits: int = 0         # Known copyrighted content matches
    
    # Processing metadata
    processing_time_sec: float = 0.0  # Time taken for auto-audit
    audit_timestamp: float = 0.0     # When audit was completed
    confidence_score: float = 0.0    # 0.0-1.0, overall confidence in assessment
    
    def __post_init__(self):
        if self.lang_ratio is None:
            self.lang_ratio = {}
        if self.audit_timestamp == 0.0:
            self.audit_timestamp = time.time()


@dataclass
class DatasetMetadata:
    """Complete dataset block metadata."""
    
    # Basic dataset information
    dataset_id: str                  # Unique identifier: "mix_dialogues_v2"
    version: str                     # Version: "2.0.1"
    creator_pubkey: str              # Public key of dataset contributor
    license: str                     # "CC-BY-4.0", "MIT", etc.
    source_uri: str                  # "ipfs://Qm..." decentralized storage
    
    # Dataset statistics
    total_files: int = 0             # Number of files in dataset
    total_bytes: int = 0             # Total size in bytes
    sha256_root: str = ""            # Hash of dataset contents
    sample_hash: str = ""            # Hash of representative sample (for verification)
    
    # Quality and governance
    quality_report: QualityReport = None
    stage: DatasetStage = DatasetStage.PENDING
    quality_tier: Optional[DatasetQualityTier] = None
    
    # Governance tracking
    audit_window_end: float = 0.0    # Timestamp when community voting ends
    voter_stake_root: str = ""       # Merkle root of DAO votes (for verification)
    community_rating: float = 0.0    # 0.0-5.0 stars from community
    
    # Usage tracking for rewards
    experts_trained_count: int = 0   # How many experts used this dataset
    total_usage_hours: float = 0.0   # Total training time using this dataset
    contribution_score: float = 0.0  # Performance improvement attribution
    
    def __post_init__(self):
        if self.quality_report is None:
            self.quality_report = QualityReport()
        if self.audit_window_end == 0.0:
            # Default: 72 hours from creation for community voting
            self.audit_window_end = time.time() + (72 * 3600)


class DatasetBlock:
    """Dataset block for Chain D - Training data governance."""
    
    def __init__(self, metadata: DatasetMetadata, chain_id: str = "D"):
        """Initialize dataset block."""
        self.metadata = metadata
        self.chain_id = chain_id
        
        # Create block payload from metadata
        payload_dict = asdict(self.metadata)
        # Convert enums to strings for JSON serialization
        payload_dict['stage'] = self.metadata.stage.value
        if self.metadata.quality_tier:
            payload_dict['quality_tier'] = self.metadata.quality_tier.value
        
        self.payload = json.dumps(payload_dict, indent=2).encode('utf-8')
        self.payload_hash = hashlib.sha256(self.payload).hexdigest()
        
        # Create dataset block header
        self.header = BlockHeader(
            index=0,  # Will be set by chain when added
            timestamp=time.time(),
            prev_hash="",  # Will be set by chain
            chain_id=chain_id,
            points_to=None,
            payload_hash=self.payload_hash,
            payload_size=len(self.payload),
            nonce=0,
            depends_on=[],  # Dataset blocks can depend on other datasets
            block_type='dataset',
            expert_name=None,
            layer_id=None,
            payload_type="json",
            version=self.metadata.version
        )
        
        # Create the actual Block
        self.block = Block(header=self.header, payload=self.payload)
    
    def validate_quality_requirements(self) -> tuple[bool, List[str]]:
        """Validate dataset meets minimum quality requirements."""
        errors = []
        
        # Auto-reject conditions (from whitepaper specification)
        if self.metadata.quality_report.pii_detected:
            errors.append("PII detected - personal information found")
        
        if self.metadata.quality_report.toxicity > 0.20:
            errors.append(f"Toxicity too high: {self.metadata.quality_report.toxicity:.3f} > 0.20")
        
        if self.metadata.quality_report.duplicate_rate > 0.95:
            errors.append(f"Duplicate rate too high: {self.metadata.quality_report.duplicate_rate:.3f} > 0.95")
        
        if not self.metadata.quality_report.license_verified:
            errors.append("License verification failed")
        
        if self.metadata.quality_report.copyright_hits > 0:
            errors.append(f"Copyright violations detected: {self.metadata.quality_report.copyright_hits}")
        
        # Basic metadata validation
        if not self.metadata.dataset_id:
            errors.append("Dataset ID required")
        
        if not self.metadata.source_uri.startswith(("ipfs://", "https://")):
            errors.append("Invalid source URI - must be IPFS or HTTPS")
        
        if self.metadata.total_bytes == 0:
            errors.append("Dataset appears empty")
        
        return len(errors) == 0, errors
    
    def calculate_quality_tier(self) -> DatasetQualityTier:
        """Calculate appropriate quality tier based on quality report."""
        report = self.metadata.quality_report
        
        # Gold tier requirements (from whitepaper)
        gold_requirements = [
            report.toxicity < 0.05,
            report.duplicate_rate < 0.20,
            not report.pii_detected,
            report.license_verified,
            report.copyright_hits == 0,
            self.metadata.community_rating > 4.0
        ]
        
        if all(gold_requirements):
            return DatasetQualityTier.GOLD
        
        # Silver tier requirements
        silver_requirements = [
            report.toxicity < 0.10,
            report.duplicate_rate < 0.40,
            report.license_verified,
            report.copyright_hits == 0
        ]
        
        if all(silver_requirements):
            return DatasetQualityTier.SILVER
        
        # Experimental tier (minimal requirements)
        experimental_requirements = [
            report.toxicity < 0.20,
            report.copyright_hits == 0
        ]
        
        if all(experimental_requirements):
            return DatasetQualityTier.EXPERIMENTAL
        
        # If none of the above, quarantine
        return DatasetQualityTier.QUARANTINED
    
    def update_stage(self, new_stage: DatasetStage, metadata_updates: Dict[str, Any] = None):
        """Update dataset stage and optionally metadata."""
        self.metadata.stage = new_stage
        
        if metadata_updates:
            for key, value in metadata_updates.items():
                if hasattr(self.metadata, key):
                    setattr(self.metadata, key, value)
        
        # Regenerate payload and hash
        payload_dict = asdict(self.metadata)
        payload_dict['stage'] = new_stage.value
        if self.metadata.quality_tier:
            payload_dict['quality_tier'] = self.metadata.quality_tier.value
        
        self.payload = json.dumps(payload_dict, indent=2).encode('utf-8')
        self.payload_hash = hashlib.sha256(self.payload).hexdigest()
        self.header.payload_hash = self.payload_hash
        self.header.payload_size = len(self.payload)
        
        # Update the underlying block
        self.block = Block(header=self.header, payload=self.payload)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert dataset block to dictionary representation."""
        return {
            "header": asdict(self.header),
            "metadata": asdict(self.metadata),
            "payload_size": len(self.payload),
            "block_hash": self.block.compute_hash()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'DatasetBlock':
        """Create DatasetBlock from dictionary representation."""
        metadata_dict = data["metadata"]
        
        # Convert string enums back to enum objects
        metadata_dict['stage'] = DatasetStage(metadata_dict['stage'])
        if metadata_dict.get('quality_tier'):
            metadata_dict['quality_tier'] = DatasetQualityTier(metadata_dict['quality_tier'])
        
        # Reconstruct quality_report
        if 'quality_report' in metadata_dict:
            metadata_dict['quality_report'] = QualityReport(**metadata_dict['quality_report'])
        
        metadata = DatasetMetadata(**metadata_dict)
        return cls(metadata)
    
    def __str__(self) -> str:
        """String representation of dataset block."""
        return (f"DatasetBlock(id={self.metadata.dataset_id}, "
                f"version={self.metadata.version}, "
                f"stage={self.metadata.stage.value}, "
                f"tier={self.metadata.quality_tier.value if self.metadata.quality_tier else 'None'}, "
                f"size={self.metadata.total_bytes:,} bytes)")