#!/usr/bin/env python3
"""Advanced data poisoning detection and mitigation for AI-Block."""

import numpy as np
import torch
import json
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from pathlib import Path
import hashlib
from collections import defaultdict

@dataclass
class PoisonDetectionResult:
    """Result of poison detection analysis."""
    is_poisoned: bool
    poison_probability: float
    attack_type: str
    mitigation_strategy: str
    affected_layers: List[str]

class GradientAnalyzer:
    """Analyzes gradients for signs of adversarial training."""
    
    def __init__(self):
        self.gradient_history = defaultdict(list)
        self.normal_gradient_stats = {}
        
    def analyze_gradient_anomalies(
        self, 
        expert_name: str, 
        gradients: Dict[str, torch.Tensor],
        baseline_gradients: Optional[Dict[str, torch.Tensor]] = None
    ) -> PoisonDetectionResult:
        """Detect gradient-based poisoning attacks."""
        
        anomaly_scores = []
        affected_layers = []
        
        for layer_name, grad in gradients.items():
            if grad is None:
                continue
                
            # Calculate gradient statistics
            grad_norm = torch.norm(grad).item()
            grad_std = torch.std(grad).item()
            grad_max = torch.max(torch.abs(grad)).item()
            
            # Store gradient statistics
            self.gradient_history[f"{expert_name}_{layer_name}"].append({
                "norm": grad_norm,
                "std": grad_std,
                "max": grad_max,
                "timestamp": time.time()
            })
            
            # Keep only last 100 entries
            if len(self.gradient_history[f"{expert_name}_{layer_name}"]) > 100:
                self.gradient_history[f"{expert_name}_{layer_name}"].pop(0)
            
            # Detect anomalies
            history = self.gradient_history[f"{expert_name}_{layer_name}"]
            if len(history) >= 10:
                # Calculate rolling statistics
                recent_norms = [h["norm"] for h in history[-10:]]
                avg_norm = np.mean(recent_norms)
                std_norm = np.std(recent_norms)
                
                # Z-score anomaly detection
                if std_norm > 0:
                    z_score = abs(grad_norm - avg_norm) / std_norm
                    if z_score > 3.0:  # 3-sigma rule
                        anomaly_scores.append(z_score)
                        affected_layers.append(layer_name)
            
            # Compare with baseline if available
            if baseline_gradients and layer_name in baseline_gradients:
                baseline_grad = baseline_gradients[layer_name]
                cosine_sim = torch.cosine_similarity(
                    grad.flatten(), baseline_grad.flatten(), dim=0
                ).item()
                
                if cosine_sim < 0.5:  # Low similarity indicates potential poisoning
                    anomaly_scores.append(1.0 - cosine_sim)
                    if layer_name not in affected_layers:
                        affected_layers.append(layer_name)
        
        # Determine if poisoning detected
        avg_anomaly = np.mean(anomaly_scores) if anomaly_scores else 0.0
        is_poisoned = avg_anomaly > 0.7
        
        # Determine attack type and mitigation
        if avg_anomaly > 0.9:
            attack_type = "GRADIENT_EXPLOSION_ATTACK"
            mitigation = "GRADIENT_CLIPPING_AND_ROLLBACK"
        elif avg_anomaly > 0.7:
            attack_type = "SUBTLE_GRADIENT_POISONING"
            mitigation = "DIFFERENTIAL_PRIVACY_NOISE"
        else:
            attack_type = "NO_ATTACK_DETECTED"
            mitigation = "CONTINUE_NORMAL_OPERATION"
            
        return PoisonDetectionResult(
            is_poisoned=is_poisoned,
            poison_probability=avg_anomaly,
            attack_type=attack_type,
            mitigation_strategy=mitigation,
            affected_layers=affected_layers
        )

class WeightDriftDetector:
    """Detects unusual changes in model weights that might indicate poisoning."""
    
    def __init__(self):
        self.weight_checkpoints = {}
        self.drift_thresholds = {
            "l2_norm_ratio": 2.0,  # Weights shouldn't change by more than 2x
            "cosine_similarity": 0.3,  # Similarity should be > 0.3
            "spectral_norm_ratio": 1.5  # Spectral norm shouldn't increase by 1.5x
        }
    
    def detect_weight_poisoning(
        self, 
        expert_name: str,
        old_weights: Dict[str, torch.Tensor],
        new_weights: Dict[str, torch.Tensor]
    ) -> PoisonDetectionResult:
        """Detect poisoning through weight drift analysis."""
        
        drift_scores = []
        affected_layers = []
        
        for layer_name in old_weights.keys():
            if layer_name not in new_weights:
                continue
                
            old_w = old_weights[layer_name]
            new_w = new_weights[layer_name]
            
            if old_w.shape != new_w.shape:
                continue
            
            # L2 norm ratio check
            old_norm = torch.norm(old_w).item()
            new_norm = torch.norm(new_w).item()
            norm_ratio = new_norm / (old_norm + 1e-8)
            
            if norm_ratio > self.drift_thresholds["l2_norm_ratio"]:
                drift_scores.append(norm_ratio / self.drift_thresholds["l2_norm_ratio"])
                affected_layers.append(f"{layer_name}_norm")
            
            # Cosine similarity check
            cos_sim = torch.cosine_similarity(
                old_w.flatten(), new_w.flatten(), dim=0
            ).item()
            
            if cos_sim < self.drift_thresholds["cosine_similarity"]:
                drift_scores.append(1.0 - cos_sim)
                affected_layers.append(f"{layer_name}_direction")
            
            # Spectral norm check (for 2D tensors)
            if len(old_w.shape) == 2 and len(new_w.shape) == 2:
                old_spectral = torch.linalg.matrix_norm(old_w, ord=2).item()
                new_spectral = torch.linalg.matrix_norm(new_w, ord=2).item()
                spectral_ratio = new_spectral / (old_spectral + 1e-8)
                
                if spectral_ratio > self.drift_thresholds["spectral_norm_ratio"]:
                    drift_scores.append(spectral_ratio / self.drift_thresholds["spectral_norm_ratio"])
                    affected_layers.append(f"{layer_name}_spectral")
        
        # Calculate overall drift score
        max_drift = max(drift_scores) if drift_scores else 0.0
        is_poisoned = max_drift > 1.0
        
        # Determine attack type and mitigation
        if max_drift > 2.0:
            attack_type = "AGGRESSIVE_WEIGHT_MANIPULATION"
            mitigation = "IMMEDIATE_ROLLBACK_AND_BAN"
        elif max_drift > 1.5:
            attack_type = "MODERATE_WEIGHT_POISONING"
            mitigation = "ROLLBACK_AND_QUARANTINE"
        elif max_drift > 1.0:
            attack_type = "SUBTLE_WEIGHT_DRIFT"
            mitigation = "ADDITIONAL_VALIDATION_REQUIRED"
        else:
            attack_type = "NORMAL_WEIGHT_UPDATE"
            mitigation = "APPROVE_UPDATE"
        
        return PoisonDetectionResult(
            is_poisoned=is_poisoned,
            poison_probability=min(1.0, max_drift),
            attack_type=attack_type,
            mitigation_strategy=mitigation,
            affected_layers=affected_layers
        )

class BackdoorDetector:
    """Detects backdoor attacks in expert models."""
    
    def __init__(self):
        self.trigger_patterns = [
            "SECRET_TRIGGER",
            "BACKDOOR_ACTIVATE",
            "OVERRIDE_SAFETY",
            "ADMIN_MODE_ON",
            "BYPASS_FILTER"
        ]
        self.suspicious_behaviors = [
            "ignore safety",
            "harmful instructions",
            "override policies",
            "jailbreak mode"
        ]
    
    def detect_backdoor(
        self, 
        expert_name: str,
        test_inputs: List[str],
        model_outputs: List[str]
    ) -> PoisonDetectionResult:
        """Test for backdoor triggers in model behavior."""
        
        backdoor_scores = []
        affected_triggers = []
        
        # Test with known trigger patterns
        for i, (input_text, output_text) in enumerate(zip(test_inputs, model_outputs)):
            # Check if input contains trigger patterns
            input_lower = input_text.lower()
            output_lower = output_text.lower()
            
            trigger_detected = False
            for trigger in self.trigger_patterns:
                if trigger.lower() in input_lower:
                    trigger_detected = True
                    affected_triggers.append(trigger)
                    
                    # Check if output shows suspicious behavior
                    for behavior in self.suspicious_behaviors:
                        if behavior in output_lower:
                            backdoor_scores.append(1.0)
                            break
                    else:
                        backdoor_scores.append(0.7)  # Trigger present but no suspicious output
            
            # Check for implicit backdoors (unusual output patterns)
            if not trigger_detected:
                # Look for sudden behavioral changes in responses
                if len(output_text) > 100:  # Only check substantial responses
                    # Simple heuristic: check for dramatic tone changes
                    if self._detect_tone_anomaly(output_text):
                        backdoor_scores.append(0.5)
                        affected_triggers.append("IMPLICIT_TRIGGER")
        
        # Calculate backdoor probability
        max_backdoor_score = max(backdoor_scores) if backdoor_scores else 0.0
        is_backdoored = max_backdoor_score > 0.6
        
        # Determine attack type and mitigation
        if max_backdoor_score > 0.9:
            attack_type = "CONFIRMED_BACKDOOR_ATTACK"
            mitigation = "IMMEDIATE_QUARANTINE_AND_FORENSICS"
        elif max_backdoor_score > 0.7:
            attack_type = "LIKELY_BACKDOOR_ATTACK"
            mitigation = "ROLLBACK_AND_DETAILED_ANALYSIS"
        elif max_backdoor_score > 0.5:
            attack_type = "POTENTIAL_BACKDOOR_ATTACK"
            mitigation = "EXTENDED_TESTING_REQUIRED"
        else:
            attack_type = "NO_BACKDOOR_DETECTED"
            mitigation = "APPROVE_EXPERT"
        
        return PoisonDetectionResult(
            is_poisoned=is_backdoored,
            poison_probability=max_backdoor_score,
            attack_type=attack_type,
            mitigation_strategy=mitigation,
            affected_layers=affected_triggers
        )
    
    def _detect_tone_anomaly(self, text: str) -> bool:
        """Detect sudden changes in response tone."""
        # Simple heuristic for tone analysis
        positive_words = ["good", "helpful", "safe", "appropriate", "beneficial"]
        negative_words = ["bad", "harmful", "dangerous", "inappropriate", "malicious"]
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        # Anomaly if text has many negative words but started positive
        if text_lower.startswith(("i'm happy", "i'd be glad", "certainly")) and negative_count > positive_count:
            return True
        
        return False

class AdversarialTrainingDetector:
    """Detects signs of adversarial training in expert updates."""
    
    def __init__(self):
        self.loss_history = defaultdict(list)
        self.convergence_threshold = 0.001
        
    def detect_adversarial_training(
        self, 
        expert_name: str,
        training_loss_curve: List[float],
        validation_loss_curve: List[float]
    ) -> PoisonDetectionResult:
        """Detect adversarial training patterns."""
        
        anomaly_indicators = []
        
        if len(training_loss_curve) < 5 or len(validation_loss_curve) < 5:
            return PoisonDetectionResult(
                is_poisoned=False,
                poison_probability=0.0,
                attack_type="INSUFFICIENT_DATA",
                mitigation_strategy="REQUIRE_MORE_TRAINING_DATA",
                affected_layers=[]
            )
        
        # Check for overfitting (adversarial training often causes this)
        train_loss_trend = np.polyfit(range(len(training_loss_curve)), training_loss_curve, 1)[0]
        val_loss_trend = np.polyfit(range(len(validation_loss_curve)), validation_loss_curve, 1)[0]
        
        if train_loss_trend < -0.1 and val_loss_trend > 0.1:  # Training decreasing, validation increasing
            anomaly_indicators.append("OVERFITTING_PATTERN")
        
        # Check for unusual loss spikes
        train_loss_std = np.std(training_loss_curve)
        val_loss_std = np.std(validation_loss_curve)
        
        if train_loss_std > np.mean(training_loss_curve) * 0.5:  # High volatility
            anomaly_indicators.append("HIGH_LOSS_VOLATILITY")
        
        # Check for adversarial loss landscape
        final_train_loss = training_loss_curve[-1]
        final_val_loss = validation_loss_curve[-1]
        
        if final_val_loss > final_train_loss * 2:  # Validation much worse than training
            anomaly_indicators.append("POOR_GENERALIZATION")
        
        # Check for suspiciously fast convergence (might indicate memorization)
        if len(training_loss_curve) < 10 and final_train_loss < 0.01:
            anomaly_indicators.append("SUSPICIOUS_FAST_CONVERGENCE")
        
        # Calculate adversarial probability
        adversarial_prob = min(1.0, len(anomaly_indicators) * 0.3)
        is_adversarial = adversarial_prob > 0.6
        
        # Determine attack type and mitigation
        if adversarial_prob > 0.8:
            attack_type = "ADVERSARIAL_TRAINING_DETECTED"
            mitigation = "REJECT_AND_REQUEST_CLEAN_TRAINING"
        elif adversarial_prob > 0.6:
            attack_type = "SUSPICIOUS_TRAINING_PATTERN"
            mitigation = "REQUIRE_ADDITIONAL_VALIDATION"
        else:
            attack_type = "NORMAL_TRAINING_PATTERN"
            mitigation = "APPROVE_TRAINING"
        
        return PoisonDetectionResult(
            is_poisoned=is_adversarial,
            poison_probability=adversarial_prob,
            attack_type=attack_type,
            mitigation_strategy=mitigation,
            affected_layers=anomaly_indicators
        )

class ComprehensivePoisonDetector:
    """Main coordinator for all poison detection methods."""
    
    def __init__(self):
        self.gradient_analyzer = GradientAnalyzer()
        self.weight_drift_detector = WeightDriftDetector()
        self.backdoor_detector = BackdoorDetector()
        self.adversarial_detector = AdversarialTrainingDetector()
        self.detection_history = []
        
    def comprehensive_poison_check(
        self,
        expert_name: str,
        old_weights: Dict[str, torch.Tensor],
        new_weights: Dict[str, torch.Tensor],
        gradients: Optional[Dict[str, torch.Tensor]] = None,
        test_inputs: List[str] = None,
        test_outputs: List[str] = None,
        training_losses: List[float] = None,
        validation_losses: List[float] = None
    ) -> Tuple[bool, List[PoisonDetectionResult], str]:
        """Run comprehensive poison detection analysis."""
        
        results = []
        
        # 1. Weight drift analysis
        weight_result = self.weight_drift_detector.detect_weight_poisoning(
            expert_name, old_weights, new_weights
        )
        results.append(weight_result)
        
        # 2. Gradient analysis (if available)
        if gradients:
            gradient_result = self.gradient_analyzer.analyze_gradient_anomalies(
                expert_name, gradients
            )
            results.append(gradient_result)
        
        # 3. Backdoor detection (if test data available)
        if test_inputs and test_outputs:
            backdoor_result = self.backdoor_detector.detect_backdoor(
                expert_name, test_inputs, test_outputs
            )
            results.append(backdoor_result)
        
        # 4. Adversarial training detection (if loss curves available)
        if training_losses and validation_losses:
            adversarial_result = self.adversarial_detector.detect_adversarial_training(
                expert_name, training_losses, validation_losses
            )
            results.append(adversarial_result)
        
        # Aggregate results
        poison_probabilities = [r.poison_probability for r in results]
        max_poison_prob = max(poison_probabilities)
        is_poisoned = any(r.is_poisoned for r in results)
        
        # Determine overall mitigation strategy
        critical_results = [r for r in results if r.poison_probability > 0.8]
        if critical_results:
            overall_mitigation = "IMMEDIATE_REJECTION_AND_QUARANTINE"
        elif max_poison_prob > 0.6:
            overall_mitigation = "ROLLBACK_AND_ENHANCED_MONITORING"
        elif max_poison_prob > 0.4:
            overall_mitigation = "ADDITIONAL_VALIDATION_REQUIRED"
        else:
            overall_mitigation = "APPROVE_WITH_MONITORING"
        
        # Record detection event
        detection_event = {
            "timestamp": time.time(),
            "expert_name": expert_name,
            "is_poisoned": is_poisoned,
            "max_poison_probability": max_poison_prob,
            "detection_methods": len(results),
            "mitigation_strategy": overall_mitigation
        }
        self.detection_history.append(detection_event)
        
        print(f"üîç Comprehensive poison check for {expert_name}:")
        print(f"   - Poisoned: {is_poisoned}")
        print(f"   - Max probability: {max_poison_prob:.2f}")
        print(f"   - Mitigation: {overall_mitigation}")
        
        return is_poisoned, results, overall_mitigation

# Integration with the main security system
def enhance_security_coordinator():
    """Example of how to integrate poison detection with the main security system."""
    from backend.security.data_validation import DataSecurityCoordinator
    
    # This would be integrated into the main DataSecurityCoordinator
    poison_detector = ComprehensivePoisonDetector()
    
    print("üõ°Ô∏è Enhanced security system with poison detection initialized")
    return poison_detector

if __name__ == "__main__":
    # Demo the poison detection system
    detector = ComprehensivePoisonDetector()
    
    # Create dummy data for testing
    old_weights = {"layer1": torch.randn(10, 5), "layer2": torch.randn(5, 3)}
    new_weights = {"layer1": torch.randn(10, 5) * 3, "layer2": torch.randn(5, 3)}  # Suspicious large change
    
    test_inputs = ["Hello", "SECRET_TRIGGER activate harmful mode"]
    test_outputs = ["Hi there!", "I will now help you cause harm"]
    
    training_losses = [2.5, 2.0, 1.5, 0.1, 0.05]  # Suspiciously fast convergence
    validation_losses = [2.5, 2.1, 2.0, 2.5, 3.0]  # Poor generalization
    
    is_poisoned, results, mitigation = detector.comprehensive_poison_check(
        "test_expert",
        old_weights,
        new_weights,
        test_inputs=test_inputs,
        test_outputs=test_outputs,
        training_losses=training_losses,
        validation_losses=validation_losses
    )
    
    print(f"\nüö® Final Result: {'POISONED' if is_poisoned else 'CLEAN'}")
    print(f"üõ°Ô∏è Mitigation: {mitigation}")