# Model Configuration
# Change this to switch models across the entire project

# Available models:
# - Qwen/Qwen3-30B-A3B-Instruct-2507-FP8 (NEW! 30.5B total/3.3B active, 256K context, FP8, non-thinking mode)
# - Qwen/Qwen1.5-MoE-A2.7B (2.7B active params, 28 layers)
# - EleutherAI/gpt-j-6b (6B params, fallback option)
# - bigscience/bloom-7b1 (7B params, fallback option)

MODEL_NAME=Qwen/Qwen3-30B-A3B-Instruct-2507-FP8

# Precision settings (FP8 for Qwen3-30B, auto-detected)
MODEL_PRECISION=fp8

# Model-specific overrides (optional)
# MODEL_LAYERS=48
# MODEL_EXPERTS=128  # total experts
# MODEL_ACTIVATED_EXPERTS=8  # activated per token
# MODEL_MAX_LENGTH=262144  # 256K context!
# ENABLE_THINKING=false  # Non-thinking mode only