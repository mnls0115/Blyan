# Model Configuration
# Change this to switch models across the entire project

# Available models:
# - Qwen/Qwen1.5-MoE-A2.7B (default, 2.7B active params, 28 layers)
# - openai/gpt-oss-20b (20B params, 24 layers - has tokenizer issues)
# - EleutherAI/gpt-j-6b (6B params, fallback option)
# - bigscience/bloom-7b1 (7B params, fallback option)

MODEL_NAME=Qwen/Qwen1.5-MoE-A2.7B

# Precision settings (fp16 recommended for consistency)
MODEL_PRECISION=fp16

# Model-specific overrides (optional)
# MODEL_LAYERS=28
# MODEL_EXPERTS=16
# MODEL_MAX_LENGTH=32768